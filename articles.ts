import { Article } from './types';

export const articlesData: Article[] = [
  {
    id: "rational_analysis",
    title: "Rational Analysis",
    url: "https://oecs.mit.edu/pub/lurik5dk/release/1?readingCollection=9dd2a47d",
    content: "Rational analysis is an approach to understanding human cognition that treats it as optimized to solve problems given certain constraints. For example, a rational analysis of memory starts from the idea that memory systems are optimized to the statistics of how the items to be remembered occur in the environment. Constructing a rational analysis requires specifying the goals of the cognitive system and then developing a formal model that instantiates those goals within the organism’s environment. In contrast to many approaches to computational modeling in cognitive science, rational analysis makes limited assumptions about the nature of the organism’s computational processes or resources. The approach of modeling the mind as performing optimal statistical inferences for a particular environment is tightly linked to the broader tradition of Bayesian cognitive modeling. \n\nHistory\n\nThe term “rational analysis” was introduced by John R. Anderson (; ), but it had many intellectual predecessors. There was a long tradition of treating human behavior as adapted to the structure of the environment (e.g., ; ). Evolutionary analysis in general treats all aspects of organisms as shaped by their environmental history, and there had been successful treatments of an animal’s behavior as arising from optimization to the structure of its environment. For example, optimal foraging theory concerns the best way to search for resources under assumptions about their distribution in the environment (). There were already explicit evolutionary explanations of aspects of cognition prior to the introduction of rational analysis (e.g., ).\n\nAnderson’s two most developed applications were for predicting (1) experimental results in memory from a model of how demands for particular stimuli appear in the environment () and (2) results in categorization experiments from a model of how features are associated with categories. There have since been rational analyses of many other cognitive domains, including the Wason selection task (), the approximate number system (), and curiosity behaviors (), to name a few. Many of these efforts have involved extensive Bayesian analyses and can be seen as part of a growing approach that uses Bayesian models of cognition () [see Bayesian Models of Cognition; Bayesianism].\n\nCore concepts\n\nRational analysis is typically described as a program of research that involves the following six steps:\n\nPrecisely specify the goals of the cognitive system.\n\nDevelop a formal model of the environment to which the system is adapted.\n\nMake the minimal assumptions about computational limitations.\n\nDerive the optimal behavioral function given items one through three.\n\nExamine the empirical literature to see if the predictions of the behavioral function are confirmed.\n\nIf the predictions are off, then iterate.\n\nPredictions of behavior can be derived from statistical models of the environment (step two). For instance, major effects in human memory can be predicted from a plausible mathematical model of the environmental distribution of memory requests ().\n\nEarly rational analyses involved only minimal computational assumptions (step three) in an effort to focus on environmental models. For instance, the assumption in the memory model was simply that memories become available in a serial order. This focus on the environment rather than the nature of computations inside individual minds led to rational analysis being construed as in opposition to more conventional mechanistic approaches that focused on specific psychological processes. A mechanistic approach would describe the processes that determined how quickly a memory became available; in contrast, rational approaches focused on when they ought to be available based on the typical demands on agents in a specific environment.\n\nThe either/or opposition between rational analysis and mechanistic approaches has disappeared. For instance, the ACT-R (Adaptive Control of Cognition–Rational) architecture () is a process-based model of cognition, but it uses rational analysis to guide its mechanistic assumptions. For instance, the activation computations in ACT-R’s declarative memory—the computations that determine the strength of a memory—were based on the rational analysis of memory. Rational analysis-based activation computations have been adopted by other theories of cognition as well (e.g., ; )."
  },
  {
    id: "marr_levels",
    title: "Levels of Analysis (Marr)",
    url: "https://oecs.mit.edu/pub/zw60p83x/release/1",
    content: "David Marr’s ‘Levels of Analysis’ is a seminal framework in cognitive science that proposes three distinct levels for understanding any information-processing system. \n\n1. The Computational Level: This is the most abstract level, focusing on 'what' the system does and 'why'. It defines the goal of the computation, its appropriateness, and the logic of the strategy by which it can be carried out. In vision, for example, this involves asking what the visual system is trying to infer from light patterns (e.g., the 3D structure of the world).\n\n2. The Algorithmic (or Representational) Level: This level asks 'how' the computation is performed. It focuses on the representations used for the input and output and the algorithm for the transformation. It bridges the gap between the abstract goal and the physical machine.\n\n3. The Implementation Level: This level describes how the representation and algorithm are physically realized. In biological systems, this involves neurons, synapses, and neural circuits. In a computer, it involves transistors and electrical signals.\n\nMarr argued that while these levels are loosely coupled, they are distinct. One can understand the nature of a computation (level 1) without necessarily understanding the specific algorithm (level 2) or the hardware (level 3). However, a complete understanding of the mind requires integrating all three."
  },
  {
    id: "computational_theory_mind",
    title: "The Computational Theory of Mind",
    url: "https://oecs.mit.edu/pub/u870vxpu/release/1",
    content: "The Computational Theory of Mind (CTM) posits that the human mind is a computational system that is realized (i.e., physically implemented) by neural activity in the brain. CTM can be summarized by the slogan: 'The mind is to the brain as software is to hardware'. \n\nCentral to CTM is the idea that thoughts are mental representations—physical states with semantic content—and that thinking is the algorithmic manipulation of these representations according to syntactic rules. This view was famously championed by philosopher Jerry Fodor, who argued for a 'Language of Thought' (Mentalese). \n\nCTM attempts to explain how rational thought is possible in a physical universe. It suggests that if thoughts are symbols, and if the transitions between thoughts track the logical relations between their contents (due to the syntactic properties of the symbols), then a physical machine can be a rational thinker. This theory forms the foundation of classical artificial intelligence and much of cognitive psychology, treating cognition as symbol processing."
  },
  {
    id: "connectionism",
    title: "Connectionism",
    url: "https://oecs.mit.edu/pub/zp5n8ivs/release/1",
    content: "Connectionism is an approach to the study of human cognition that utilizes mathematical models known as artificial neural networks. Unlike the classical Computational Theory of Mind, which views cognition as the serial processing of discrete symbols, connectionism views mental phenomena as the emergent processes of interconnected networks of simple units.\n\nKey features of connectionist models include:\n- Parallel Distributed Processing (PDP): Information is processed simultaneously across a large number of units.\n- Distributed Representation: Concepts are not stored in single 'grandmother cells' but are represented by a pattern of activation across many units.\n- Learning by Weight Adjustment: Knowledge is stored in the strength of connections (weights) between units. Learning occurs by adjusting these weights (e.g., via backpropagation) to reduce error.\n\nConnectionism rose to prominence in the 1980s as a reaction to the brittleness of symbolic AI. It offers a more biologically plausible account of cognition, excelling at tasks like pattern recognition, generalization, and learning from fuzzy data, though it has faced criticism regarding its ability to handle compositional structure and systematic reasoning."
  },
  {
    id: "history_of_ai",
    title: "A Short History of AI",
    url: "https://ai100.stanford.edu/2016-report/appendix-i-short-history-ai",
    content: "The history of Artificial Intelligence (AI) is a saga of high hopes, disappointing setbacks, and renewed enthusiasm. \n\nThe Birth (1950s): The field was formally founded at the Dartmouth Conference in 1956, where John McCarthy coined the term 'Artificial Intelligence'. Early pioneers like Newell and Simon created programs that could prove theorems, leading to optimism that machines would soon match human intelligence.\n\nEarly Realities (1960s-1970s): While early programs solved micro-worlds, they struggled with the complexity and ambiguity of the real world. Funding dried up in the mid-70s, leading to the first 'AI Winter'.\n\nExpert Systems (1980s): AI boomed again with the rise of 'Expert Systems'—programs encoded with the specific knowledge of human experts (e.g., medical diagnosis). However, these systems were brittle and expensive to maintain, leading to a second AI Winter in the late 80s.\n\nThe Rise of Machine Learning (1990s-Present): A shift occurred from trying to program intelligence directly to creating systems that could learn from data. Probabilistic methods and machine learning took center stage. In the 2010s, the combination of 'Big Data', powerful GPUs, and Deep Learning (multi-layered neural networks) led to breakthroughs in image recognition, natural language processing (e.g., ChatGPT), and game playing (e.g., AlphaGo), ushering in the current era of AI prominence."
  },
  {
    id: "neural_networks_noc",
    title: "Nature of Code: Neural Networks",
    url: "https://natureofcode.com/neural-networks/",
    content: "Neural networks are computational models inspired by the human brain. Just as the brain consists of billions of neurons connected by synapses, an artificial neural network consists of 'nodes' (artificial neurons) connected by 'weights'.\n\nThe Perceptron: The simplest neural network is the Perceptron, invented by Frank Rosenblatt in 1958. It takes several inputs, multiplies them by weights, sums them up, and passes the result through an activation function to produce an output. It can learn to classify simple linear patterns.\n\nMultilayer Perceptrons: To solve complex, non-linear problems, neurons are organized into layers: an input layer, one or more 'hidden' layers, and an output layer. This architecture allows the network to learn complex features and representations.\n\nLearning: The magic of neural networks lies in their ability to learn. This is typically done through a process called 'Supervised Learning' using an algorithm like Backpropagation. The network makes a guess, compares it to the correct answer, calculates the error, and then adjusts the weights backwards through the layers to minimize that error next time. This iterative process allows the network to 'learn' functions from data."
  }
];
